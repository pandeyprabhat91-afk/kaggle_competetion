{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7b2f52f",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries\n",
    "Major Prabhat Pandey, DA25M002 | MTech AI & DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07a73661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully - Ready for model training\n"
     ]
    }
   ],
   "source": [
    "# Libraries for data manipulation and machine learning\n",
    "# Learned during MTech coursework - applying ensemble methods\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# ML Models - Using gradient boosting variants for regression\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(\"Libraries imported successfully - Ready for model training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3263fb0a",
   "metadata": {},
   "source": [
    "### Step 2: Load Data\n",
    "Preprocessed embeddings (768-dimensional) from google/embeddinggemma-300m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a5bb9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data from: c:\\Users\\lonew\\OneDrive\\Desktop\\KAGGLE CHALLENGE\\final_submission\\train_data_fixed_embeddings.json\n",
      "Loading test data from: c:\\Users\\lonew\\OneDrive\\Desktop\\KAGGLE CHALLENGE\\final_submission\\test_data_fixed_embeddings.json\n",
      "Loading test data from: c:\\Users\\lonew\\OneDrive\\Desktop\\KAGGLE CHALLENGE\\final_submission\\test_data_fixed_embeddings.json\n",
      "Loading metric embeddings from: c:\\Users\\lonew\\OneDrive\\Desktop\\KAGGLE CHALLENGE\\final_submission\\metric_name_embeddings.npy\n",
      "\n",
      "Data loaded successfully:\n",
      "   Training samples: 5,000\n",
      "   Test samples: 3,638\n",
      "   Metric embeddings shape: (145, 768)\n",
      "Loading metric embeddings from: c:\\Users\\lonew\\OneDrive\\Desktop\\KAGGLE CHALLENGE\\final_submission\\metric_name_embeddings.npy\n",
      "\n",
      "Data loaded successfully:\n",
      "   Training samples: 5,000\n",
      "   Test samples: 3,638\n",
      "   Metric embeddings shape: (145, 768)\n"
     ]
    }
   ],
   "source": [
    "# Define file paths for training and test datasets\n",
    "# Organized workspace structure for reproducibility\n",
    "WORKSPACE_ROOT = Path(__file__).parent if '__file__' in globals() else Path.cwd()\n",
    "TRAIN_DATA_PATH = WORKSPACE_ROOT / \"train_data_fixed_embeddings.json\"\n",
    "TEST_DATA_PATH = WORKSPACE_ROOT / \"test_data_fixed_embeddings.json\"\n",
    "METRIC_EMBEDDINGS_PATH = WORKSPACE_ROOT / \"metric_name_embeddings.npy\"\n",
    "\n",
    "print(f\"Loading training data from: {TRAIN_DATA_PATH}\")\n",
    "with open(TRAIN_DATA_PATH, 'r', encoding='utf-8') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "print(f\"Loading test data from: {TEST_DATA_PATH}\")\n",
    "with open(TEST_DATA_PATH, 'r', encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(f\"Loading metric embeddings from: {METRIC_EMBEDDINGS_PATH}\")\n",
    "metric_embeddings = np.load(METRIC_EMBEDDINGS_PATH)\n",
    "\n",
    "print(f\"\\nData loaded successfully:\")\n",
    "print(f\"   Training samples: {len(train_data):,}\")\n",
    "print(f\"   Test samples: {len(test_data):,}\")\n",
    "print(f\"   Metric embeddings shape: {metric_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab9a8fd",
   "metadata": {},
   "source": [
    "### Step 3: Feature Engineering\n",
    "Siamese network features: 7,680 dimensions (base + differences + products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40e34208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training features with Siamese architecture...\n",
      "Creating test features...\n",
      "Creating test features...\n",
      "\n",
      "Features created:\n",
      "   Training shape: (5000, 6912)\n",
      "   Test shape: (3638, 6912)\n",
      "   Training target mean: 9.120, std: 0.942\n",
      "\n",
      "Features created:\n",
      "   Training shape: (5000, 6912)\n",
      "   Test shape: (3638, 6912)\n",
      "   Training target mean: 9.120, std: 0.942\n"
     ]
    }
   ],
   "source": [
    "def create_siamese_features(metric_emb, prompt_emb, response_emb):\n",
    "    \"\"\"\n",
    "    Create Siamese network-style features from embeddings.\n",
    "    Technique inspired by similarity learning architectures.\n",
    "    \n",
    "    Args:\n",
    "        metric_emb: Metric name embedding (768d)\n",
    "        prompt_emb: Prompt embedding (768d)\n",
    "        response_emb: Response embedding (768d)\n",
    "    \n",
    "    Returns:\n",
    "        Combined feature vector (7,680d)\n",
    "    \n",
    "    Note: This captures both individual embeddings and their relationships\n",
    "    through difference and product operations \n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Base features - preserving original embedding information (3,072 dimensions)\n",
    "    features.append(metric_emb)    # 768d\n",
    "    features.append(prompt_emb)    # 768d\n",
    "    features.append(response_emb)  # 768d\n",
    "    \n",
    "    # Siamese features - capturing embedding relationships (4,608 dimensions)\n",
    "    features.append(metric_emb - prompt_emb)           # 768d - directional difference\n",
    "    features.append(metric_emb - response_emb)         # 768d - directional difference\n",
    "    features.append(np.abs(metric_emb - prompt_emb))   # 768d - magnitude of difference\n",
    "    features.append(np.abs(metric_emb - response_emb)) # 768d - magnitude of difference\n",
    "    features.append(metric_emb * prompt_emb)           # 768d - element-wise interaction\n",
    "    features.append(metric_emb * response_emb)         # 768d - element-wise interaction\n",
    "    \n",
    "    return np.concatenate(features)\n",
    "\n",
    "def prepare_features(data, metric_embeddings):\n",
    "    \"\"\"\n",
    "    Prepare feature matrix from raw data samples.\n",
    "    Vectorized approach for efficiency on large datasets.\n",
    "    \"\"\"\n",
    "    # Create metric name to index mapping\n",
    "    with open(WORKSPACE_ROOT / \"metric_names.json\", 'r', encoding='utf-8') as f:\n",
    "        metric_names = json.load(f)\n",
    "    metric_to_id = {name: idx for idx, name in enumerate(metric_names)}\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for sample in data:\n",
    "        # Get metric embedding using metric name\n",
    "        metric_name = sample['metric_name']\n",
    "        metric_idx = metric_to_id[metric_name]\n",
    "        metric_emb = metric_embeddings[metric_idx]\n",
    "        prompt_emb = np.array(sample['prompt_embedding'])\n",
    "        response_emb = np.array(sample['response_embedding'])\n",
    "        \n",
    "        # Generate feature vector using Siamese architecture\n",
    "        features = create_siamese_features(metric_emb, prompt_emb, response_emb)\n",
    "        X.append(features)\n",
    "        \n",
    "        if 'score' in sample:\n",
    "            y.append(float(sample['score']))  # Convert to float\n",
    "    \n",
    "    return np.array(X), np.array(y) if y else None\n",
    "\n",
    "print(\"Creating training features with Siamese architecture...\")\n",
    "X_train, y_train = prepare_features(train_data, metric_embeddings)\n",
    "\n",
    "print(\"Creating test features...\")\n",
    "X_test, _ = prepare_features(test_data, metric_embeddings)\n",
    "\n",
    "print(f\"\\nFeatures created:\")\n",
    "print(f\"   Training shape: {X_train.shape}\")\n",
    "print(f\"   Test shape: {X_test.shape}\")\n",
    "print(f\"   Training target mean: {y_train.mean():.3f}, std: {y_train.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8178ff54",
   "metadata": {},
   "source": [
    "### Step 4: Train Ensemble Models\n",
    "5 models: CatBoost, XGBoost, LightGBM, Random Forest, CatBoost Deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2807383c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model 1/5: CatBoost (Siamese features)\n",
      "   Model 1 trained successfully\n",
      "   Model 1 trained successfully\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "\n",
    "# Model 1: CatBoost with moderate depth\n",
    "# Hyperparameters tuned through experimentation\n",
    "# GPU acceleration enabled for faster training\n",
    "print(\"Training Model 1/5: CatBoost (Siamese features)\")\n",
    "model1 = CatBoostRegressor(\n",
    "    iterations=2000,\n",
    "    learning_rate=0.03,\n",
    "    depth=6,\n",
    "    l2_leaf_reg=3,\n",
    "    random_seed=42,\n",
    "    task_type='GPU',\n",
    "    devices='0',\n",
    "    verbose=False\n",
    ")\n",
    "model1.fit(X_train, y_train)\n",
    "models.append(('CatBoost_Siamese', model1))\n",
    "print(\"   Model 1 trained successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26c51660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model 2/5: XGBoost (Siamese features)\n",
      "   Model 2 trained successfully\n",
      "   Model 2 trained successfully\n"
     ]
    }
   ],
   "source": [
    "# Model 2: XGBoost with matching hyperparameters\n",
    "# Using same depth and regularization for diversity\n",
    "# GPU acceleration enabled for faster training\n",
    "print(\"Training Model 2/5: XGBoost (Siamese features)\")\n",
    "model2 = XGBRegressor(\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.03,\n",
    "    max_depth=6,\n",
    "    reg_lambda=3,\n",
    "    random_state=42,\n",
    "    tree_method='gpu_hist',\n",
    "    gpu_id=0,\n",
    "    verbosity=0\n",
    ")\n",
    "model2.fit(X_train, y_train)\n",
    "models.append(('XGBoost_Siamese', model2))\n",
    "print(\"   Model 2 trained successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67c133f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model 3/5: LightGBM (Siamese features)\n",
      "   Model 3 trained successfully\n",
      "   Model 3 trained successfully\n"
     ]
    }
   ],
   "source": [
    "# Model 3: LightGBM for faster training\n",
    "# Efficient on high-dimensional feature space\n",
    "# GPU acceleration enabled for faster training\n",
    "print(\"Training Model 3/5: LightGBM (Siamese features)\")\n",
    "model3 = LGBMRegressor(\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.03,\n",
    "    max_depth=6,\n",
    "    reg_lambda=3,\n",
    "    random_state=42,\n",
    "    device='gpu',\n",
    "    gpu_platform_id=0,\n",
    "    gpu_device_id=0,\n",
    "    verbose=-1\n",
    ")\n",
    "model3.fit(X_train, y_train)\n",
    "models.append(('LightGBM_Siamese', model3))\n",
    "print(\"   Model 3 trained successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7870cd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model 4/5: Random Forest (Basic features)\n",
      "   Model 4 trained successfully\n",
      "   Model 4 trained successfully\n"
     ]
    }
   ],
   "source": [
    "# Model 4: Random Forest as baseline\n",
    "# Different architecture provides ensemble diversity\n",
    "print(\"Training Model 4/5: Random Forest (Basic features)\")\n",
    "model4 = RandomForestRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "model4.fit(X_train, y_train)\n",
    "models.append(('RandomForest_Basic', model4))\n",
    "print(\"   Model 4 trained successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d72b85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model 5/5: CatBoost Deep (All features)\n",
      "   Model 5 trained successfully\n",
      "\n",
      "All 5 models trained - ensemble ready for predictions\n",
      "   Model 5 trained successfully\n",
      "\n",
      "All 5 models trained - ensemble ready for predictions\n"
     ]
    }
   ],
   "source": [
    "# Model 5: Deep CatBoost model\n",
    "# Increased depth captures complex non-linear patterns\n",
    "# Using CPU to avoid GPU memory issues with deeper model\n",
    "print(\"Training Model 5/5: CatBoost Deep (All features)\")\n",
    "model5 = CatBoostRegressor(\n",
    "    iterations=2000,\n",
    "    learning_rate=0.03,\n",
    "    depth=8,\n",
    "    l2_leaf_reg=5,\n",
    "    random_seed=42,\n",
    "    task_type='CPU',\n",
    "    thread_count=-1,\n",
    "    verbose=False\n",
    ")\n",
    "model5.fit(X_train, y_train)\n",
    "models.append(('CatBoost_Deep', model5))\n",
    "print(\"   Model 5 trained successfully\")\n",
    "\n",
    "print(f\"\\nAll 5 models trained - ensemble ready for predictions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1936534",
   "metadata": {},
   "source": [
    "### Step 5: Generate Ensemble Predictions\n",
    "Simple averaging across all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "692e7370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions from each model...\n",
      "\n",
      "   CatBoost_Siamese: Train RMSE = 0.4104\n",
      "   CatBoost_Siamese: Train RMSE = 0.4104\n",
      "   XGBoost_Siamese: Train RMSE = 0.1089\n",
      "   XGBoost_Siamese: Train RMSE = 0.1089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   LightGBM_Siamese: Train RMSE = 0.1105\n",
      "   RandomForest_Basic: Train RMSE = 0.7129\n",
      "   RandomForest_Basic: Train RMSE = 0.7129\n",
      "   CatBoost_Deep: Train RMSE = 0.1500\n",
      "\n",
      "Creating ensemble (simple average)...\n",
      "\n",
      "Ensemble created:\n",
      "   Ensemble Train RMSE: 0.2566\n",
      "   Ensemble Test Mean: 9.104\n",
      "   Ensemble Test Std: 0.296\n",
      "   CatBoost_Deep: Train RMSE = 0.1500\n",
      "\n",
      "Creating ensemble (simple average)...\n",
      "\n",
      "Ensemble created:\n",
      "   Ensemble Train RMSE: 0.2566\n",
      "   Ensemble Test Mean: 9.104\n",
      "   Ensemble Test Std: 0.296\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating predictions from each model...\\n\")\n",
    "train_predictions = []\n",
    "test_predictions = []\n",
    "\n",
    "# Generate predictions and evaluate each model\n",
    "for name, model in models:\n",
    "    train_pred = model.predict(X_train)\n",
    "    test_pred = model.predict(X_test)\n",
    "    \n",
    "    train_predictions.append(train_pred)\n",
    "    test_predictions.append(test_pred)\n",
    "    \n",
    "    # Calculate individual model performance\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n",
    "    print(f\"   {name}: Train RMSE = {train_rmse:.4f}\")\n",
    "\n",
    "# Create ensemble using simple average\n",
    "# Equal weighting performed best in validation experiments\n",
    "print(\"\\nCreating ensemble (simple average)...\")\n",
    "ensemble_train = np.mean(train_predictions, axis=0)\n",
    "ensemble_test = np.mean(test_predictions, axis=0)\n",
    "\n",
    "ensemble_train_rmse = np.sqrt(mean_squared_error(y_train, ensemble_train))\n",
    "print(f\"\\nEnsemble created:\")\n",
    "print(f\"   Ensemble Train RMSE: {ensemble_train_rmse:.4f}\")\n",
    "print(f\"   Ensemble Test Mean: {ensemble_test.mean():.3f}\")\n",
    "print(f\"   Ensemble Test Std: {ensemble_test.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3797367",
   "metadata": {},
   "source": [
    "### Step 6: Apply Spread Calibration\n",
    "Linear calibration: alpha=1.00, beta=-0.2 | Floor and clip to [0,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8464b519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration parameters:\n",
      "   Alpha (spread): 1.0\n",
      "   Beta (bias): -0.2\n",
      "\n",
      "Calibrating test predictions...\n",
      "\n",
      "Calibration complete:\n",
      "   Before calibration: mean=9.104, std=0.296\n",
      "   After calibration: mean=8.360, std=0.502\n",
      "   Range: [2, 9]\n"
     ]
    }
   ],
   "source": [
    "# Optimal calibration parameters found through experimentation\n",
    "ALPHA = 1.00   # Spread scaling factor\n",
    "BETA = -0.2    # Bias adjustment\n",
    "\n",
    "print(f\"Calibration parameters:\")\n",
    "print(f\"   Alpha (spread): {ALPHA}\")\n",
    "print(f\"   Beta (bias): {BETA}\")\n",
    "\n",
    "def apply_spread_calibration(predictions, alpha, beta):\n",
    "    \"\"\"\n",
    "    Apply spread calibration formula to raw predictions.\n",
    "    \n",
    "    Process:\n",
    "    1. Linear transformation: predictions * alpha + beta\n",
    "    2. Floor to integer values\n",
    "    3. Clip to valid range [0, 10]\n",
    "    \n",
    "    This ensures predictions match the expected score distribution\n",
    "    while maintaining ranking order.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Apply linear transformation\n",
    "    calibrated = predictions * alpha + beta\n",
    "    \n",
    "    # Floor to integer for discrete scores\n",
    "    calibrated = np.floor(calibrated)\n",
    "    \n",
    "    # Clip to valid scoring range\n",
    "    calibrated = np.clip(calibrated, 0, 10).astype(int)\n",
    "    \n",
    "    return calibrated\n",
    "\n",
    "# Apply calibration to test predictions\n",
    "print(\"\\nCalibrating test predictions...\")\n",
    "test_calibrated = apply_spread_calibration(ensemble_test, ALPHA, BETA)\n",
    "\n",
    "print(f\"\\nCalibration complete:\")\n",
    "print(f\"   Before calibration: mean={ensemble_test.mean():.3f}, std={ensemble_test.std():.3f}\")\n",
    "print(f\"   After calibration: mean={test_calibrated.mean():.3f}, std={test_calibrated.std():.3f}\")\n",
    "print(f\"   Range: [{test_calibrated.min()}, {test_calibrated.max()}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330bb5ae",
   "metadata": {},
   "source": [
    "### Step 7: Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ced8ced3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created: submission.csv\n",
      "   Rows: 3,638\n",
      "   Columns: ['id', 'score']\n",
      "\n",
      "Submission statistics:\n",
      "   Mean: 8.360\n",
      "   Std: 0.502\n",
      "   Min: 2\n",
      "   Max: 9\n"
     ]
    }
   ],
   "source": [
    "# Generate test IDs since test data doesn't include them\n",
    "# Kaggle submission typically needs sequential IDs starting from 0\n",
    "test_ids = list(range(len(test_data)))\n",
    "\n",
    "# Create submission DataFrame with required format\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'score': test_calibrated\n",
    "})\n",
    "\n",
    "# Save to CSV file\n",
    "OUTPUT_PATH = 'submission.csv'\n",
    "submission.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(f\"Submission file created: {OUTPUT_PATH}\")\n",
    "print(f\"   Rows: {len(submission):,}\")\n",
    "print(f\"   Columns: {list(submission.columns)}\")\n",
    "\n",
    "# Display submission statistics\n",
    "print(f\"\\nSubmission statistics:\")\n",
    "print(f\"   Mean: {submission['score'].mean():.3f}\")\n",
    "print(f\"   Std: {submission['score'].std():.3f}\")\n",
    "print(f\"   Min: {submission['score'].min()}\")\n",
    "print(f\"   Max: {submission['score'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efc8cc0",
   "metadata": {},
   "source": [
    "### Step 8: Score Distribution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
